{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LegionXF23/DevSoc-Assign/blob/main/Copy_of_LLM_Response_Generator_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: The API key is usually loaded from an environment variable for security.\n",
        "# For this demonstration, leave it as an empty string. The platform will handle it.\n",
        "API_KEY = \"\"\n",
        "API_URL_TEMPLATE = \"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}\"\n",
        "MODEL_NAME = \"gemini-2.5-flash-preview-09-2025\"\n",
        "\n",
        "INPUT_FILE_NAME = \"input_prompts.txt\"\n",
        "OUTPUT_FILE_PREFIX = \"llm_responses_\"\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_DELAY = 2  # seconds\n",
        "# ---------------------\n",
        "\n",
        "def call_gemini_api(prompt, model=MODEL_NAME):\n",
        "    \"\"\"\n",
        "    Makes a POST request to the Gemini API with exponential backoff.\n",
        "    Returns the generated text or None on persistent failure.\n",
        "    \"\"\"\n",
        "    url = API_URL_TEMPLATE.format(model=model, key=API_KEY)\n",
        "\n",
        "    # Payload for the API request\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "        # Using Google Search grounding for up-to-date information\n",
        "        \"tools\": [{\"google_search\": {}}],\n",
        "        \"systemInstruction\": {\n",
        "            \"parts\": [{\n",
        "                \"text\": \"You are a helpful, professional, and concise assistant. Answer the user's query directly.\"\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    delay = INITIAL_DELAY\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            print(f\"-> Attempting to call API for prompt: '{prompt[:50]}...' (Attempt {attempt + 1}/{MAX_RETRIES})\")\n",
        "\n",
        "            response = requests.post(\n",
        "                url,\n",
        "                headers={'Content-Type': 'application/json'},\n",
        "                data=json.dumps(payload),\n",
        "                timeout=30  # Timeout after 30 seconds\n",
        "            )\n",
        "\n",
        "            # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse the JSON response\n",
        "            result = response.json()\n",
        "            candidate = result.get('candidates', [{}])[0]\n",
        "\n",
        "            # Extract the text content\n",
        "            generated_text = candidate.get('content', {}).get('parts', [{}])[0].get('text')\n",
        "\n",
        "            if generated_text:\n",
        "                print(\"<- Success.\")\n",
        "                return generated_text\n",
        "            else:\n",
        "                # Handle cases where the API returns a success status but no text content\n",
        "                print(\"<- API returned successfully, but no text was generated. Retrying.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # Handle network errors, timeouts, or bad HTTP status codes\n",
        "            print(f\"<- Error on attempt {attempt + 1}: {e}\")\n",
        "\n",
        "        # If this was the last attempt, break the loop\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2 # Exponential backoff\n",
        "        else:\n",
        "            print(\"<- Max retries reached. Failing this prompt.\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "def process_prompts(input_file):\n",
        "    \"\"\"Reads prompts from the input file and processes them via the LLM API.\"\"\"\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: Input file '{input_file}' not found.\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Read prompts, stripping empty lines and whitespace\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    total_prompts = len(prompts)\n",
        "    print(f\"\\nFound {total_prompts} prompts to process.\")\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"\\n--- Processing Prompt {i + 1}/{total_prompts} ---\")\n",
        "\n",
        "        # Get the LLM response\n",
        "        response_text = call_gemini_api(prompt)\n",
        "\n",
        "        # Store the result object\n",
        "        results.append({\n",
        "            \"id\": i + 1,\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response_text if response_text else \"API Call Failed or returned no content.\"\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_results(data, prefix):\n",
        "    \"\"\"Saves the list of results to a JSON file with a timestamp.\"\"\"\n",
        "    # Generate a timestamped file name (YYYYMMDD_HHMMSS)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f\"{prefix}{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"\\n--- Process Complete ---\")\n",
        "        print(f\"Successfully saved {len(data)} results to: {output_filename}\")\n",
        "        return output_filename\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"--- LLM Response Generator using {MODEL_NAME} ---\")\n",
        "\n",
        "    # 1. Process prompts and get responses\n",
        "    response_data = process_prompts(INPUT_FILE_NAME)\n",
        "\n",
        "    # 2. Save the results\n",
        "    if response_data:\n",
        "        save_results(response_data, OUTPUT_FILE_PREFIX)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "E8Z7D-E3yquM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}