{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LegionXF23/DevSoc-Assign/blob/main/Copy_of_Lin_Reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "LvkxJLK872xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJEzeBaYrp3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylSYM7srAaTK"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8TzpYrAaTK"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko0DUQMIAaTL"
      },
      "source": [
        "### **Exploring the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIvXaDLAaTL"
      },
      "source": [
        "Let's start with loading the training data from the csv into a pandas dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Load the datasets from GitHub. Train dataset has already been loaded for you in df below. To get test dataset use the commented code."
      ],
      "metadata": {
        "id": "CeVVY-uHrsqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj_ZCCgTAaTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "7939c0b8-fcd2-44fa-be25-fd888f364876",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1585328834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/train_processed_splitted.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/train_processed_splitted.csv')\n",
        "print(\"--- Exploring the dataset ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-4Ujl1AaTL"
      },
      "source": [
        "Let's see what the first 5 rows of this dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ieOW_eQAaTL"
      },
      "outputs": [],
      "source": [
        "print(\"First 5 rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNnfJbohAaTM"
      },
      "source": [
        "What are all the features present? What is the range for each of the features along with their mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0e6ZqO6AaTM"
      },
      "outputs": [],
      "source": [
        "print(\"\\nFeatures (Columns) present:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nRange (Min/Max) and Mean for each feature:\")\n",
        "feature_stats = df.describe().T[['min', 'max', 'mean']]\n",
        "feature_stats['range'] = feature_stats['max'] - feature_stats['min']\n",
        "print(feature_stats[['range', 'min', 'max', 'mean']])\n",
        "print(\"-\" * 35)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hjdd5Y5AaTM"
      },
      "source": [
        "### **Feature Scaling and One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ls8u3hAaTM"
      },
      "source": [
        "You must have noticed that some features `(such as Utilities)` are not continuous values.\n",
        "  \n",
        "These features contain values indicating different categories and must somehow be converted to numbers so that the computer can understand it. `(Computers only understand numbers and not strings)`\n",
        "  \n",
        "These features are called categorical features. We can represent these features as a `One-Hot Representation`\n",
        "  \n",
        "  \n",
        "You must have also noticed that all the other features, each are in a different scale. This can be detremental to the performance of our linear regression model and so we normalize them so that all of them are in the range $[0,1]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeoczqQIAaTM"
      },
      "source": [
        "> NOTE: When you are doing feature scaling, store the min/max which you will use to normalize somewhere. This is then to be used at testing time. Try to think why are doing this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17LZP2qAaTN"
      },
      "outputs": [],
      "source": [
        "# Do the one-hot encoding here\n",
        "print(\"--- Feature Scaling and One-Hot Encoding ---\")\n",
        "\n",
        "y_train_original = df['SalePrice'].copy()\n",
        "X_train = df.drop('SalePrice', axis=1)\n",
        "\n",
        "categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols, dtype='int32')\n",
        "\n",
        "if 'Utilities' in X_train.columns:\n",
        "    X_train = X_train.drop('Utilities', axis=1)\n",
        "\n",
        "# Fill any remaining NaN values with 0 (Common practice after one-hot encoding and before scaling)\n",
        "X_train.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG7rV9TCAaTN"
      },
      "outputs": [],
      "source": [
        "# Do the feature scaling here\n",
        "min_vals = X_train.min(axis=0)\n",
        "max_vals = X_train.max(axis=0)\n",
        "# Avoid division by zero for columns with constant values (range=0)\n",
        "range_vals = max_vals - min_vals\n",
        "range_vals[range_vals == 0] = 1 # Set range to 1 for constant columns\n",
        "X_train_scaled = (X_train - min_vals) / range_vals\n",
        "\n",
        "# Scale Target (SalePrice)\n",
        "target_min = y_train_original.min()\n",
        "target_max = y_train_original.max()\n",
        "y_train_scaled = (y_train_original - target_min) / (target_max - target_min)\n",
        "\n",
        "\n",
        "print(f\"Shape of scaled Features (X): {X_train_scaled.shape}\")\n",
        "print(f\"Shape of scaled Target (Y): {y_train_scaled.shape}\")\n",
        "print(\"-\" * 35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcZxb-V4AaTN"
      },
      "source": [
        "### **Conversion to NumPy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-_3l_1iAaTN"
      },
      "source": [
        "Ok so now that we have all preprocessed all the data, we need to convert it to numpy for our linear regression model\n",
        "  \n",
        "Assume that our dataset has a total of $N$ datapoints. Each datapoint having a total of $D$ features (after one-hot encoding), we want our numpy array to be of shape $(N, D)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJmGKlEgAaTN"
      },
      "source": [
        "In our task, we have to predict the `SalePrice`. We will need 2 numpy arrays $\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "(X, Y)$. These represent the features and targets respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJk8M5QmAaTN"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy array\n",
        "print(\"--- Conversion to NumPy ---\")\n",
        "X = X_train_scaled.to_numpy()\n",
        "Y = y_train_scaled.to_numpy().reshape(-1, 1)\n",
        "N, D = X.shape\n",
        "print(f\"N (Datapoints): {N}\")\n",
        "print(f\"D (Features after encoding): {D}\")\n",
        "print(\"-\" * 35)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7YT1X-3AaTN"
      },
      "source": [
        "## Linear Regression formulation\n",
        "  \n",
        "We now have our data in the form we need. Let's try to create a linear model to get our initial (Really bad) prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgXFnXgAaTN"
      },
      "source": [
        "Let's say a single datapoint in our dataset consists of 3 features $(x_1, x_2, x_3)$, we can pose it as a linear equation as follows:\n",
        "$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$\n",
        "Here we have to learn 4 parameters $(w_1, w_2, w_3, b)$\n",
        "  \n",
        "  \n",
        "Now how do we extend this to multiple datapoints?  \n",
        "  \n",
        "  \n",
        "Try to answer the following:\n",
        "- How many parameters will we have to learn in the cae of our dataset? (Don't forget the bias term)\n",
        "- Form a linear equation for our dataset. We need just a single matrix equation which correctly represents all the datapoints in our dataset\n",
        "- Implement the linear equation as an equation using NumPy arrays (Start by randomly initializing the weights from a standard normal distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvB5DGT7AaTO"
      },
      "outputs": [],
      "source": [
        "print(\"--- Linear Regression formulation ---\")\n",
        "num_parameters = D + 1\n",
        "print(f\"Number of parameters to learn: D (Features) + 1 (Bias) = {D} + 1 = {num_parameters}\")\n",
        "W = np.random.randn(D, 1) * 0.01\n",
        "b = np.random.randn(1) * 0.01\n",
        "Y_pred_initial = X @ W + b\n",
        "\n",
        "print(f\"Initial Weights (W) shape: {W.shape}\")\n",
        "print(f\"Initial Bias (b) value: {b}\")\n",
        "print(f\"Initial Prediction (Y_pred) shape: {Y_pred_initial.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldc0Ap_AaTO"
      },
      "source": [
        "How well does our model perform? Try comparing our predictions with the actual values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnnX7hG5AaTO"
      },
      "outputs": [],
      "source": [
        "initial_mse_loss = np.mean((Y_pred_initial - Y)**2)\n",
        "print(f\"Initial MSE Loss (Scaled): {initial_mse_loss:.6f}\")\n",
        "print(\"-\" * 35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MryM_jtQAaTO"
      },
      "source": [
        "### **Learning weights using gradient descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9rq3T9AaTO"
      },
      "source": [
        "So these results are really horrible. We need to somehow update our weights so that it correclty represents our data. How do we do that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EKd0mhkAaTO"
      },
      "source": [
        "We must do the following:\n",
        "- We need some numerical indication for our performance, for this we define a Loss Function ( $\\mathscr{L}$ )\n",
        "- Find the gradients of the `Loss` with respect to the `Weights`\n",
        "- Update the weights in accordance to the gradients: $W = W - \\alpha\\nabla_W \\mathscr{L}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUfs8wh3AaTO"
      },
      "source": [
        "Lets define the loss function:\n",
        "- We will use the MSE loss since it is a regression task. (Specify the assumptions we make while doing so as taught in the class).\n",
        "- Implement this loss as a function. (Use numpy as much as possible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4G28aKRAaTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "52dc1e19-53e4-4c16-d276-c1e300c7fd08",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-957558406.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-957558406.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def mse_loss_fn(y_true, y_pred):\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_fn(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error (MSE) loss.\n",
        "    Assumptions: The errors (residuals) are normally distributed,\n",
        "                 homoscedastic (constant variance), and independent.\n",
        "    \"\"\"\n",
        "    # y_true and y_pred are (N, 1) matrices\n",
        "    N = y_true.shape[0]\n",
        "    loss = np.sum((y_pred - y_true)**2) / N\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnb9WpfAaTP"
      },
      "source": [
        "Calculate the gradients of the loss with respect to the weights (and biases). First write the equations down on a piece of paper, then proceed to implement it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDYpZjjMAaTP"
      },
      "outputs": [],
      "source": [
        "def get_gradients(y_true, y_pred, W, b, X):\n",
        "    \"\"\"\n",
        "    Calculates the gradients for the MSE loss function with respect to the weights (W) and bias (b).\n",
        "\n",
        "    Gradient of Loss (L) w.r.t W: $\\nabla_W \\mathscr{L} = \\frac{2}{N} X^T (Y_{pred} - Y_{true})$\n",
        "    Gradient of Loss (L) w.r.t b: $\\nabla_b \\mathscr{L} = \\frac{2}{N} \\sum (Y_{pred} - Y_{true})$\n",
        "    \"\"\"\n",
        "    N = y_true.shape[0]\n",
        "    error = y_pred - y_true # (N, 1)\n",
        "\n",
        "    # dW: (D, N) @ (N, 1) -> (D, 1)\n",
        "    dW = (2/N) * (X.T @ error)\n",
        "\n",
        "    # db: scalar\n",
        "    db = (2/N) * np.sum(error)\n",
        "\n",
        "    return dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbjlNBXAaTP"
      },
      "source": [
        "Update the weights using the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xstl9DpAaTP"
      },
      "outputs": [],
      "source": [
        "def update(weights, bias, gradients_weights, gradients_bias, lr):\n",
        "    \"\"\"\n",
        "    Updates the weights (and bias) using the gradients and the learning rate.\n",
        "    $W_{new} = W - \\alpha \\nabla_W \\mathscr{L}$\n",
        "    $b_{new} = b - \\alpha \\nabla_b \\mathscr{L}$\n",
        "    \"\"\"\n",
        "    weights_new = weights - lr * gradients_weights\n",
        "    bias_new = bias - lr * gradients_bias\n",
        "    return weights_new, bias_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZqXDTjFAaTP"
      },
      "source": [
        "Put all these together to find the loss value, its gradient and finally updating the weights in a loop. Feel free to play around with different learning rates and epochs\n",
        "  \n",
        "> NOTE: The code in comments are just meant to be used as a guide. You will have to do changes based on your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3BVX4nlAaTP"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1000\n",
        "LEARNING_RATE = 2e-2\n",
        "\n",
        "# Re-initialize weights and bias for training\n",
        "W = np.random.randn(D, 1) * 0.01\n",
        "b = np.random.randn(1) * 0.01\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Model prediction: Y_pred = X @ W + b\n",
        "    Y_pred = X @ W + b # (N, D) @ (D, 1) -> (N, 1)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = mse_loss_fn(Y, Y_pred)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Calculate gradients\n",
        "    dW, db = get_gradients(Y, Y_pred, W, b, X)\n",
        "\n",
        "    # Update weights and bias\n",
        "    W, b = update(W, b, dW, db, LEARNING_RATE)\n",
        "\n",
        "    # Print loss periodically\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {loss:.6f}\")\n",
        "\n",
        "final_train_loss = losses[-1]\n",
        "print(f\"\\nFinal Training Loss (Scaled): {final_train_loss:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgND-OLAaTQ"
      },
      "source": [
        "Now use matplotlib to plot the loss graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxFqG5GSAaTQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"-\" * 35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaZdAzbvAaTQ"
      },
      "source": [
        "### **Testing with test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSbgtM6AaTQ"
      },
      "source": [
        "Load and apply all the preprocessing steps used in the training data for the testing data as well. Remember to use the **SAME** min/max values which you used for the training set and not recalculate them from the test set. Also mention why we are doing this.\n",
        "\n",
        "To load test data from GitHub, use the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iuQ4ulhAaTQ"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/test_processed_splitted.csv')\n",
        "print(df_test)\n",
        "\n",
        "# Let's find all the columns that are missing in the test set\n",
        "missing_cols = set(df.columns) - set(df_test.columns)\n",
        "\n",
        "# Add these columns to the test set with all zeros\n",
        "for col in missing_cols:\n",
        "    df_test[col] = 0\n",
        "\n",
        "if 'Utilities_AllPub' not in df_test.columns:\n",
        "    df_test = df_test.join(pd.get_dummies(df_test['Utilities'], dtype = 'int32', prefix = 'Utilities'))\n",
        "    df_test = df_test.drop('Utilities', axis = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxqOWigRAaTQ"
      },
      "source": [
        "Using the weights learnt above, predict the values in the test dataset. Also answer the following questions:\n",
        "- Are the predictions good?\n",
        "- What is the MSE loss for the testset\n",
        "- Is the MSE loss for testing greater or lower than training\n",
        "- Why is this the case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvF1EJfZAaTQ"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "print(\"--- Testing Data Preprocessing and Prediction ---\")\n",
        "\n",
        "# Load test data\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/test_processed_splitted.csv')\n",
        "print(\"Initial Test Data Head:\")\n",
        "print(df_test.head(2))\n",
        "\n",
        "# Separate features and target\n",
        "y_test_original = df_test['SalePrice'].copy()\n",
        "X_test = df_test.drop('SalePrice', axis=1)\n",
        "\n",
        "# Apply One-Hot Encoding to categorical columns in the test set\n",
        "categorical_cols_test = X_test.select_dtypes(include='object').columns.tolist()\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols_test, dtype='int32')\n",
        "\n",
        "# Drop the original 'Utilities' column if still present after one-hot (as per prompt logic)\n",
        "if 'Utilities' in X_test.columns:\n",
        "    X_test = X_test.drop('Utilities', axis=1)\n",
        "\n",
        "# Alignment with Training Data Columns\n",
        "# Rationale: We must ensure the test data matrix (x_test) has the same number of features (columns)\n",
        "# and that they are in the **exact same order** as the training data (X), so that each weight $W_i$\n",
        "# corresponds to the correct feature $X_i$ during matrix multiplication.\n",
        "\n",
        "X_train_cols = list(min_vals.index) # Use the columns from the training min/max\n",
        "\n",
        "# Find missing columns in X_test and add them with 0\n",
        "missing_cols = set(X_train_cols) - set(X_test.columns)\n",
        "for col in missing_cols:\n",
        "    X_test[col] = 0\n",
        "\n",
        "# Find extra columns in X_test and drop them\n",
        "extra_cols = set(X_test.columns) - set(X_train_cols)\n",
        "if extra_cols:\n",
        "    X_test = X_test.drop(columns=list(extra_cols), axis=1)\n",
        "\n",
        "# Ensure the columns are in the exact same order as training\n",
        "X_test = X_test[X_train_cols]\n",
        "# Fill NaN values\n",
        "X_test.fillna(0, inplace=True)\n",
        "\n",
        "# Scale features\n",
        "range_vals = max_vals - min_vals\n",
        "range_vals[range_vals == 0] = 1 # Handle constant columns\n",
        "X_test_scaled = (X_test - min_vals) / range_vals\n",
        "y_test_scaled = (y_test_original - target_min) / (target_max - target_min)\n",
        "\n",
        "# Convert to numpy array\n",
        "x_test = X_test_scaled.to_numpy() # (N_test, D)\n",
        "y_test = y_test_scaled.to_numpy().reshape(-1, 1) # (N_test, 1)\n",
        "\n",
        "print(f\"\\nFinal Test Features shape: {x_test.shape}\")\n",
        "print(\"-\" * 35)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''extra_cols = list(set(df_test.columns) - set(df.columns))\n",
        "print(\"Extra columns in df_test:\", extra_cols)\n",
        "\n",
        "missing_cols = list(set(df.columns) - set(df_test.columns))\n",
        "print(\"Missing columns in df_test:\", missing_cols)'''"
      ],
      "metadata": {
        "id": "MkmpTFp8yi1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "Y_pred_test = x_test @ W + b # (N_test, 1)\n",
        "\n",
        "# Calculate test loss (Scaled)\n",
        "loss_test = mse_loss_fn(y_test, Y_pred_test)\n",
        "\n",
        "# Scale the predictions and true values back to the original scale\n",
        "Y_pred_test_original = Y_pred_test * (target_max - target_min) + target_min\n",
        "Y_test_original = y_test * (target_max - target_min) + target_min\n",
        "\n",
        "# Display sample results\n",
        "idx = np.random.randint(0, x_test.shape[0], 5)\n",
        "Y_pred_test_sample = Y_pred_test_original[idx].round().astype(int)\n",
        "Y_true_test_sample = Y_test_original[idx].round().astype(int)\n",
        "\n",
        "print('Predicted SalePrice: \\t', Y_pred_test_sample.squeeze().tolist())\n",
        "print('Actual SalePrice: \\t', Y_true_test_sample.squeeze().tolist())\n",
        "print('\\nTest Loss (Scaled): \\t', loss_test)\n",
        "# Scale the predictions back to the original scale\n"
      ],
      "metadata": {
        "id": "Z-TbJp0ntTip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Model Analysis ---\")\n",
        "print(f\"Final Training Loss (Scaled): \\t {final_train_loss:.6f}\")\n",
        "print(f\"Test Loss (Scaled): \\t\\t {loss_test:.6f}\")\n",
        "\n",
        "# 1. Are the predictions good?\n",
        "print(\"\\n- **Are the predictions good?**\")\n",
        "print(f\"Based on the low scaled Test Loss of **{loss_test:.4f}**, the predictions are generally good. The low MSE indicates that the model's predictions are close to the actual scaled values, and the sample predictions are in the correct order of magnitude.\")\n",
        "\n",
        "# 2. What is the MSE loss for the testset\n",
        "print(\"\\n- **What is the MSE loss for the testset?**\")\n",
        "print(f\"The **Mean Squared Error (MSE) loss** for the test set (on scaled data) is **{loss_test:.6f}**.\")\n",
        "\n",
        "# 3. Is the MSE loss for testing greater or lower than training\n",
        "comparison = \"greater\" if loss_test > final_train_loss else \"lower\"\n",
        "print(\"\\n- **Is the MSE loss for testing greater or lower than training?**\")\n",
        "print(f\"The MSE loss for testing ({loss_test:.6f}) is **{comparison}** than the final training loss ({final_train_loss:.6f}).\")\n",
        "\n",
        "# 4. Why is this the case\n",
        "print(\"\\n- **Why is this the case?**\")\n",
        "if comparison == \"greater\":\n",
        "    print(\"It is **expected** for the test loss to be slightly greater. The model's weights were optimized specifically for the training data. The test data is unseen, and a slightly higher test loss indicates that the model is **generalizing** well to new data rather than having perfectly memorized the training set (overfitting).\")\n",
        "else:\n",
        "    print(\"If the test loss is lower, it suggests the test set is either 'easier' (less noisy or less variable) than the training set, or that the model exhibits excellent generalization, though this outcome is less common than the test loss being slightly higher.\")"
      ],
      "metadata": {
        "id": "GIGSX11u62JU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}